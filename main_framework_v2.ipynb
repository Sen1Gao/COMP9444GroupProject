{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2a0b4-0191-4eb6-baa9-3bcaac990fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision\n",
    "from torchmetrics.segmentation import MeanIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities Section\n",
    "# RGB format\n",
    "index_color_mapping={0:(0,0,0),         # void\n",
    "                     1:(108,64,20),     # dirt\n",
    "                     2:(255,229,204),   # sand\n",
    "                     3:(0,102,0),       # grass\n",
    "                     4:(0,255,0),       # tree\n",
    "                     5:(0,153,153),     # pole\n",
    "                     6:(0,128,255),     # water\n",
    "                     7:(0,0,255),       # sky\n",
    "                     8:(255,255,0),     # vehicle\n",
    "                     9:(255,0,127),     # container/generic-object\n",
    "                     10:(64,64,64),     # asphalt\n",
    "                     11:(255,128,0),    # gravel\n",
    "                     12:(255,0,0),      # building\n",
    "                     13:(153,76,0),     # mulch\n",
    "                     14:(102,102,0),    # rock-bed\n",
    "                     15:(102,0,0),      # log\n",
    "                     16:(0,255,128),    # bicycle\n",
    "                     17:(204,153,255),  # person\n",
    "                     18:(102,0,204),    # fence\n",
    "                     19:(255,153,204),  # bush\n",
    "                     20:(0,102,102),    # sign\n",
    "                     21:(153,204,255),  # rock\n",
    "                     22:(102,255,255),  # bridge\n",
    "                     23:(101,101,11),   # concrete\n",
    "                     24:(114,85,47)}    # picnic-table\n",
    "\n",
    "# RGB format\n",
    "color_index_mapping={(0,0,0):0,         # void\n",
    "                     (108,64,20):1,     # dirt\n",
    "                     (255,229,204):2,   # sand\n",
    "                     (0,102,0):3,       # grass\n",
    "                     (0,255,0):4,       # tree\n",
    "                     (0,153,153):5,     # pole\n",
    "                     (0,128,255):6,     # water\n",
    "                     (0,0,255):7,       # sky\n",
    "                     (255,255,0):8,     # vehicle\n",
    "                     (255,0,127):9,     # container/generic-object\n",
    "                     (64,64,64):10,     # asphalt\n",
    "                     (255,128,0):11,    # gravel\n",
    "                     (255,0,0):12,      # building\n",
    "                     (153,76,0):13,     # mulch\n",
    "                     (102,102,0):14,    # rock-bed\n",
    "                     (102,0,0):15,      # log\n",
    "                     (0,255,128):16,    # bicycle\n",
    "                     (204,153,255):17,  # person\n",
    "                     (102,0,204):18,    # fence\n",
    "                     (255,153,204):19,  # bush\n",
    "                     (0,102,102):20,    # sign\n",
    "                     (153,204,255):21,  # rock\n",
    "                     (102,255,255):22,  # bridge\n",
    "                     (101,101,11):23,   # concrete\n",
    "                     (114,85,47):24}    # picnic-table     \n",
    "\n",
    "def index_lookup(color:tuple)->int:\n",
    "    \"\"\"\n",
    "    Get index of color from color_index_mapping where the format of color is RGB.\\n\n",
    "    Therefore, you must convert color format to RGB before you pass variable 'color'\\n\n",
    "    The variable 'color' is a tuple.\n",
    "    \"\"\"\n",
    "    return color_index_mapping[color]\n",
    "\n",
    "def to_color_label(index_label:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert index label to color label for showing the result of prediction.\n",
    "    \"\"\"\n",
    "    h,w=index_label.shape\n",
    "    color_label=np.zeros((h,w,3),dtype=np.uint8)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            color=index_color_mapping[index_label[i][j]]\n",
    "            r,g,b=color\n",
    "            color_label[i][j]=np.array([b,g,r])\n",
    "    return color_label\n",
    "\n",
    "def get_dirs_list(dir_path:Path)->list:\n",
    "    return [p for p in dir_path.iterdir() if p.is_dir()]\n",
    "\n",
    "def get_files_list(dir_path:Path,suffix:str)->list:\n",
    "    return [f for f in dir_path.iterdir() if f.is_file() and f.suffix==suffix]\n",
    "\n",
    "def process_row(row:np.ndarray)->list:\n",
    "    index_list=[]\n",
    "    for element in row:\n",
    "        bgr=element.tolist()\n",
    "        rgb=(bgr[2],bgr[1],bgr[0])\n",
    "        index_list.append(index_lookup(rgb))\n",
    "    return index_list\n",
    "\n",
    "def convert_color2index(file_path:str)->np.ndarray:\n",
    "    color=cv2.imread(file_path,cv2.IMREAD_COLOR)\n",
    "    rows=[color[i,:] for i in range(color.shape[0])]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        rst=list(executor.map(process_row,rows))\n",
    "    return np.array(rst,dtype=np.uint8)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, workspace_path:str, csv_file_name:str, color_transforms=None, target_size=None, loading_mode=\"pre\"):\n",
    "        workspace_path=Path(workspace_path)\n",
    "        csv_file_path=workspace_path/csv_file_name\n",
    "        files_list=np.genfromtxt(csv_file_path,dtype=None,encoding='utf-8',delimiter=',')\n",
    "        self.num_files=files_list.shape[0]\n",
    "        self.color_transforms=color_transforms\n",
    "        self.target_size=target_size\n",
    "        self.loading_mode=loading_mode\n",
    "\n",
    "        self.data_file_full_path_list=[]\n",
    "        for pair in files_list:\n",
    "            color_file_path=Path(pair[0])\n",
    "            index_file_path=Path(pair[1])\n",
    "            self.data_file_full_path_list.append([color_file_path,index_file_path])\n",
    "\n",
    "        self.color_index_list=None\n",
    "        if loading_mode==\"pre\":\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                self.color_index_list=list(tqdm(executor.map(self.__load_color_and_index__,self.data_file_full_path_list),desc=\"Load images and labels\",total=self.__len__()))\n",
    "\n",
    "    def __load_color_and_index__(self,path_pair)->list:\n",
    "        color_image_path=path_pair[0]\n",
    "        index_label_path=path_pair[1]\n",
    "        color_image=cv2.imread(color_image_path.as_posix(),cv2.IMREAD_UNCHANGED)\n",
    "        index_label=cv2.imread(index_label_path.as_posix(),cv2.IMREAD_UNCHANGED)\n",
    "        if self.target_size is not None:\n",
    "            color_image=cv2.resize(color_image,self.target_size,interpolation=cv2.INTER_LINEAR)\n",
    "            index_label=cv2.resize(index_label,self.target_size,interpolation=cv2.INTER_NEAREST)\n",
    "        return[color_image,index_label]\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return self.num_files\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        color_image=None\n",
    "        index_label=None\n",
    "        if self.loading_mode==\"pre\":\n",
    "            color_image=self.color_index_list[index][0]\n",
    "            index_label=self.color_index_list[index][1]\n",
    "        else:\n",
    "            pair=self.__load_color_and_index__(self.data_file_full_path_list[index])\n",
    "            color_image=pair[0]\n",
    "            index_label=pair[1]\n",
    "            \n",
    "        if self.color_transforms is not None:\n",
    "            color_image=self.color_transforms(color_image)\n",
    "            index_label=torch.from_numpy(index_label).long()\n",
    "        return color_image,index_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d985ac-cfcf-4b9e-a921-689d048a56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before everything starting, please create a workspace folder named RUGD_ws \n",
    "# Move folders RUGD_frames-with-annotations and RUGD_annotations into RUGD_ws\n",
    "# Create folder model within RUGD_ws\n",
    "# Set workspace path here\n",
    "workspace_path=\"C:/Users/SenGao/Downloads/RUGD_ws\"\n",
    "\n",
    "frames_folder_name=\"RUGD_frames-with-annotations\"\n",
    "annotations_folder_name=\"RUGD_annotations\"\n",
    "annotations_index_folder_name=\"RUGD_annotations_index\"\n",
    "model_folder_name=\"model\"\n",
    "\n",
    "random_state=42\n",
    "random.seed(random_state)\n",
    "\n",
    "training_folder_list=[\"park-2\",\"trail\",\"trail-3\",\"trail-4\",\"trail-6\",\"trail-9\",\"trail-10\",\"trail-11\",\"trail-12\",\"trail-14\",\"trail-15\",\"village\"]\n",
    "val_folder_list=[\"park-8\",\"trail-5\"]\n",
    "testing_folder_list=[\"creek\",\"park-1\",\"trail-7\",\"trail-13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03907aa-b075-4510-94f5-39141af376a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index labels generation\n",
    "annotations_path=Path(workspace_path)/annotations_folder_name\n",
    "annotation_index_path=Path(workspace_path)/annotations_index_folder_name\n",
    "annotation_index_path.mkdir()\n",
    "print(\"Start generating index labels.\")\n",
    "dirs_list=get_dirs_list(annotations_path)\n",
    "for p in dirs_list:\n",
    "    new_p=Path(annotation_index_path/p.name)\n",
    "    new_p.mkdir()\n",
    "    files_list=get_files_list(p,\".png\")\n",
    "    for f in tqdm(files_list,desc=f\"folder '{p.name}' is being processed.\",total=len(files_list),leave=False):\n",
    "        cv2.imwrite((new_p/f.name).as_posix(),convert_color2index(f.as_posix()))\n",
    "print(\"Generating index labels finishes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50b4f7-a1c7-42d4-bff0-dba4b8f125a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset spliting\n",
    "frames_path=Path(workspace_path)/frames_folder_name\n",
    "annotations_index_path=Path(workspace_path)/annotations_index_folder_name\n",
    "saving_path=Path(workspace_path)\n",
    "\n",
    "print(\"Start spliting dataset.\")\n",
    "training_data_path_str_list=[]\n",
    "for folder_name in training_folder_list:\n",
    "    frame_path_list=get_files_list(frames_path/folder_name,\".png\")\n",
    "    for frame_path in frame_path_list:\n",
    "        index_path=annotations_index_path/frame_path.parent.name/frame_path.name\n",
    "        training_data_path_str_list.append([frame_path.as_posix(),index_path.as_posix()])\n",
    "np.savetxt((saving_path/\"train_data_path.csv\").as_posix(),np.array(training_data_path_str_list),delimiter=',',fmt='%s')\n",
    "val_data_path_str_list=[]\n",
    "for folder_name in val_folder_list:\n",
    "    frame_path_list=get_files_list(frames_path/folder_name,\".png\")\n",
    "    for frame_path in frame_path_list:\n",
    "        index_path=annotations_index_path/frame_path.parent.name/frame_path.name\n",
    "        val_data_path_str_list.append([frame_path.as_posix(),index_path.as_posix()])\n",
    "np.savetxt((saving_path/\"val_data_path.csv\").as_posix(),np.array(val_data_path_str_list),delimiter=',',fmt='%s')\n",
    "testing_data_path_str_list=[]\n",
    "for folder_name in testing_folder_list:\n",
    "    frame_path_list=get_files_list(frames_path/folder_name,\".png\")\n",
    "    for frame_path in frame_path_list:\n",
    "        index_path=annotations_index_path/frame_path.parent.name/frame_path.name\n",
    "        testing_data_path_str_list.append([frame_path.as_posix(),index_path.as_posix()])\n",
    "np.savetxt((saving_path/\"test_data_path.csv\").as_posix(),np.array(testing_data_path_str_list),delimiter=',',fmt='%s')\n",
    "print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413af9b-b2e6-4fa3-8f7a-3eeb584eb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization_para_calculating\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self,workspace_path,training_data_csv_file_name=\"train_data_path.csv\") -> None:\n",
    "        workspace_path=Path(workspace_path)\n",
    "        csv_file_path=workspace_path/training_data_csv_file_name\n",
    "        files_list= np.genfromtxt(csv_file_path.as_posix(),dtype=None,encoding='utf-8',delimiter=',')\n",
    "        self.frame_path_list=[]\n",
    "        for s in files_list[:,:1].tolist():\n",
    "            self.frame_path_list.append(Path(s[0]))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frame_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=cv2.imread(self.frame_path_list[index].as_posix(),cv2.IMREAD_UNCHANGED)\n",
    "        image=transforms.ToTensor()(image)\n",
    "        return image\n",
    "\n",
    "csv_file_name=\"train_data_path.csv\"\n",
    "dataset=TrainingDataset(workspace_path,csv_file_name)\n",
    "data_loader=DataLoader(dataset=dataset,batch_size=32)\n",
    "\n",
    "sum_bgr=torch.zeros(3,dtype=torch.float64)\n",
    "num_pixel=0\n",
    "for images in tqdm(data_loader,desc=\"Calculate mean: \"):\n",
    "    sum_bgr+=torch.sum(images,[0,2,3])\n",
    "    num_pixel+=images.size(0)*images.size(2)*images.size(3)\n",
    "mean=(sum_bgr/num_pixel)\n",
    "print(f\"mean: {mean}\")\n",
    "\n",
    "sum_squared_bgr=torch.zeros(3,dtype=torch.float64)\n",
    "for images in tqdm(data_loader,desc=\"Calculate std: \"):\n",
    "    resharped_mean=mean[None, :, None, None]\n",
    "    difference_value=images-resharped_mean\n",
    "    squared_difference_value=difference_value**2\n",
    "    sum_squared_bgr+=squared_difference_value.sum([0,2,3])\n",
    "std=torch.sqrt(sum_squared_bgr/num_pixel)\n",
    "print(f\"std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c01e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class SpatialPath(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialPath, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = ConvBlock(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = ConvBlock(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout1=nn.Dropout2d(0.1)\n",
    "        self.dropout2=nn.Dropout2d(0.2)\n",
    "        self.dropout3=nn.Dropout2d(0.3)\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.dropout1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.dropout2(x)\n",
    "        x=self.conv3(x)\n",
    "        x=self.dropout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionRefinementModule(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels):\n",
    "        super(AttentionRefinementModule, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg=self.global_pool(x)\n",
    "        avg=self.conv(avg)\n",
    "        avg=self.bn(avg)\n",
    "        attention = self.sigmoid(avg)\n",
    "        x=self.conv(x)\n",
    "        x=self.bn(x)\n",
    "        x=self.relu(x)\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class ContextPath(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContextPath, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.arm8x = AttentionRefinementModule(128,512)\n",
    "        self.arm16x = AttentionRefinementModule(256,256)\n",
    "        self.conv1=ConvBlock(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2=ConvBlock(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout1=nn.Dropout2d(0.1)\n",
    "        self.dropout2=nn.Dropout2d(0.2)\n",
    "        self.dropout3=nn.Dropout2d(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.resnet18.conv1(x)\n",
    "        x2 = self.resnet18.bn1(x2)\n",
    "        x2 = self.resnet18.relu(x2)\n",
    "        x4 = self.resnet18.maxpool(x2)\n",
    "        x4 = self.resnet18.layer1(x4)\n",
    "        x8 = self.resnet18.layer2(x4)  \n",
    "        x16 = self.resnet18.layer3(x8)\n",
    "        \n",
    "        avg=self.avg_pool(x16)\n",
    "        avg_up=F.interpolate(avg, size=x16.size()[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        x16_arm=self.arm16x(x16)\n",
    "        x16_arm=x16_arm+avg_up\n",
    "        x16_arm_up=F.interpolate(x16_arm, size=x8.size()[2:], mode='bilinear', align_corners=True)\n",
    "        x16_arm_up=self.conv1(x16_arm_up)\n",
    "        x16_arm_up=self.dropout2(x16_arm_up)\n",
    "        \n",
    "        x8_arm=self.arm8x(x8)\n",
    "        x8_arm=x8_arm+x16_arm_up\n",
    "        x8_arm=self.conv2(x8_arm)\n",
    "        x8_arm=self.dropout3(x8_arm)\n",
    "        \n",
    "        return x8_arm,x\n",
    "\n",
    "\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels, 1, 1, 0)\n",
    "        self.dropout=nn.Dropout2d(0.4)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(out_channels, out_channels // 4, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 4, out_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, sp, cp):\n",
    "        fusion = torch.cat([sp, cp], dim=1)\n",
    "        fusion = self.conv(fusion)\n",
    "        fusion=self.dropout(fusion)\n",
    "        attention = self.attention(fusion)\n",
    "        return fusion + fusion * attention\n",
    "\n",
    "\n",
    "class BiSeNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BiSeNet, self).__init__()\n",
    "        self.spatial_path = SpatialPath()\n",
    "        self.context_path = ContextPath()\n",
    "        self.feature_fusion = FeatureFusionModule(512,128)\n",
    "        self.final_conv=nn.Conv2d(128, num_classes, 1,1,0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sp = self.spatial_path(x)\n",
    "        cp,x = self.context_path(x)\n",
    "        out = self.feature_fusion(sp,cp)\n",
    "        out = F.interpolate(out, size=x.size()[2:], mode='bilinear', align_corners=True)\n",
    "        out = self.final_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95febd35-7e0f-4b53-a745-5a29e5abd5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters of training\n",
    "color_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4044, 0.4067, 0.4069],std=[0.2750, 0.2738, 0.2710])\n",
    "])\n",
    "loading_mode=\"pre\"\n",
    "num_classes=25\n",
    "target_size=(512,512)\n",
    "epochs=3\n",
    "batch_size=8\n",
    "learning_rate=0.001\n",
    "\n",
    "model=BiSeNet(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "#scheduler = optim.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "metric_val_acc = MulticlassAccuracy(num_classes=num_classes, ignore_index=0)\n",
    "metric_val_precision = MulticlassPrecision(num_classes=num_classes, average=\"macro\", ignore_index=0)\n",
    "metric_pixel_wise_acc= MulticlassAccuracy(num_classes=num_classes, ignore_index=0)\n",
    "metric_mIoU=MeanIoU(num_classes=num_classes,include_background=False,input_format=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdfb6e-a710-40b0-a87d-4a5c87d0afb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "training_dataset=CustomDataset(workspace_path,\"train_data_path.csv\",color_transforms,target_size,loading_mode)\n",
    "val_dataset=CustomDataset(workspace_path,\"val_data_path.csv\",color_transforms,target_size,loading_mode)\n",
    "training_loader=DataLoader(training_dataset,batch_size,shuffle=True,drop_last=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current used device is {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    "metric_val_acc=metric_val_acc.to(device)\n",
    "metric_val_precision=metric_val_precision.to(device)\n",
    "metric_pixel_wise_acc=metric_pixel_wise_acc.to(device)\n",
    "metric_mIoU=metric_mIoU.to(device)\n",
    "\n",
    "y_training_loss=np.zeros(epochs,dtype=np.float32)\n",
    "y_val_loss=np.zeros(epochs,dtype=np.float32)\n",
    "y_val_accuracy=np.zeros(epochs,dtype=np.float32)\n",
    "y_val_precision=np.zeros(epochs,dtype=np.float32)\n",
    "y_val_pixel_wise_acc=np.zeros(epochs,dtype=np.float32)\n",
    "y_val_mIoU=np.zeros(epochs,dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    message=f\"Current epoch:{epoch+1}/{epochs} \"\n",
    "    model.train()\n",
    "    training_loss_sum = 0.0\n",
    "    for images, labels in tqdm(training_loader, desc=message+\"Training: \", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss_sum += loss.item()\n",
    "    y_training_loss[epoch]=training_loss_sum / len(training_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=message+\"validating: \", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_sum += loss.item()\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            pred_labels_flat = pred_labels.view(pred_labels.size(0), -1)\n",
    "            labels_flat = labels.view(labels.size(0), -1)\n",
    "            metric_val_acc.update(pred_labels,labels)\n",
    "            metric_val_precision.update(pred_labels,labels)\n",
    "            metric_pixel_wise_acc.update(pred_labels_flat,labels_flat)\n",
    "            metric_mIoU.update(pred_labels,labels)\n",
    "\n",
    "    y_val_loss[epoch]=val_loss_sum/len(val_loader)\n",
    "    y_val_accuracy[epoch]=metric_val_acc.compute()\n",
    "    y_val_precision[epoch]=metric_val_precision.compute()\n",
    "    y_val_pixel_wise_acc[epoch]=metric_pixel_wise_acc.compute()\n",
    "    y_val_mIoU[epoch]=metric_mIoU.compute()\n",
    "    metric_val_acc.reset()\n",
    "    metric_val_precision.reset()\n",
    "    metric_pixel_wise_acc.reset()\n",
    "    metric_mIoU.reset()\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    print(f\"Training loss: {y_training_loss[epoch]:.4f} Validation loss: {y_val_loss[epoch]:.4f}\")\n",
    "    print(f\"accuracy: {y_val_accuracy[epoch]:.4f} Precision: {y_val_precision[epoch]:.4f}\")\n",
    "    print(f\"Pixel-wise Acc: {y_val_pixel_wise_acc[epoch]:.4f} MeanIoU: {y_val_mIoU[epoch]:.4f}\")\n",
    "torch.save(model.state_dict(), Path(workspace_path)/model_folder_name/f\"model.pth\")\n",
    "\n",
    "np.savetxt((Path(workspace_path)/model_folder_name/\"training_loss.csv\").as_posix(),y_training_loss,delimiter=',',fmt='%f')\n",
    "np.savetxt((Path(workspace_path)/model_folder_name/\"val_loss.csv\").as_posix(),y_val_loss,delimiter=',',fmt='%f')\n",
    "np.savetxt((Path(workspace_path)/model_folder_name/\"accuracy.csv\").as_posix(),y_val_accuracy,delimiter=',',fmt='%f')\n",
    "np.savetxt((Path(workspace_path)/model_folder_name/\"precision.csv\").as_posix(),y_val_precision,delimiter=',',fmt='%f')\n",
    "np.savetxt((Path(workspace_path)/model_folder_name/\"pixel_wise_acc.csv\").as_posix(),y_val_pixel_wise_acc,delimiter=',',fmt='%f')\n",
    "np.savetxt((Path(workspace_path)/model_folder_name/\"mIoU.csv\").as_posix(),y_val_mIoU,delimiter=',',fmt='%f')\n",
    "print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772da8d-e813-4f3c-ace7-79b0a7e67b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results of training\n",
    "training_loss=np.genfromtxt((Path(workspace_path)/model_folder_name/\"training_loss.csv\").as_posix(),dtype=None,encoding='utf-8',delimiter=',')\n",
    "val_loss=np.genfromtxt((Path(workspace_path)/model_folder_name/\"val_loss.csv\").as_posix(),dtype=None,encoding='utf-8',delimiter=',')\n",
    "val_accuracy=np.genfromtxt((Path(workspace_path)/model_folder_name/\"accuracy.csv\").as_posix(),dtype=None,encoding='utf-8',delimiter=',')\n",
    "val_precision=np.genfromtxt((Path(workspace_path)/model_folder_name/\"precision.csv\").as_posix(),dtype=None,encoding='utf-8',delimiter=',')\n",
    "val_pixel_wise_accuracy=np.genfromtxt((Path(workspace_path)/model_folder_name/\"pixel_wise_acc.csv\").as_posix(),dtype=None,encoding='utf-8',delimiter=',')\n",
    "val_mIoU=np.genfromtxt((Path(workspace_path)/model_folder_name/\"mIoU.csv\").as_posix(),dtype=None,encoding='utf-8',delimiter=',')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(training_loss, color='blue', label='Traning loss')\n",
    "plt.plot(val_loss, color='orange', label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_accuracy, color='black', label='Accuracy')\n",
    "plt.plot(val_precision, color='red', label='Precision')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Accuracy & Precision\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(val_pixel_wise_accuracy, color='yellow', label='pixel_acc')\n",
    "plt.plot(val_mIoU, color='green', label='mIoU')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Pixel_acc & Mean IoU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514bee3-d2ee-4e61-8d87-99fec06ea240",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Testing parameters\n",
    "# model_file_name=\"model_params_1_15.pth\"\n",
    "# # Load testing data\n",
    "# testing_dataset=CustomDataset(workspace_path,\"test_data_path.csv\",color_transforms,target_size,loading_mode)\n",
    "# testing_loader=DataLoader(testing_dataset,batch_size,shuffle=False,drop_last=True)\n",
    "# # Testing\n",
    "# torch.cuda.empty_cache()\n",
    "# model=BiSeNet(num_classes)\n",
    "# model.load_state_dict(torch.load((Path(workspace_path)/model_folder_name/model_file_name)))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# correct_pixel_sum=0\n",
    "# pixel_sum=0\n",
    "# total_intersection=np.zeros(num_classes,dtype=np.int64)\n",
    "# total_union=np.zeros(num_classes,dtype=np.int64)\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in tqdm(testing_loader, desc=\"testing: \", leave=False):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         probs = torch.softmax(outputs, dim=1)\n",
    "#         pred_labels = torch.argmax(probs, dim=1)\n",
    "#         correct_pixel_sum+=(pred_labels==labels).sum().item()\n",
    "#         pixel_sum+=labels.numel()\n",
    "#         for pred, label in zip(pred_labels, labels):\n",
    "#             for index in range(num_classes):\n",
    "#                 pred_inds = (pred == index)\n",
    "#                 label_inds = (label == index)\n",
    "#                 intersection = (pred_inds & label_inds).sum().item()\n",
    "#                 union = (pred_inds | label_inds).sum().item()\n",
    "#                 total_intersection[index] += intersection\n",
    "#                 total_union[index] += union\n",
    "# pixel_accuracy=correct_pixel_sum/pixel_sum\n",
    "# iou=total_intersection[1:]/(total_union+1e-6)[1:]\n",
    "# mIoU=iou.mean().item()\n",
    "# print(f\"pixel-wise accuracy: {pixel_accuracy:.4f} mIoU: {mIoU:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP9444GroupProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
